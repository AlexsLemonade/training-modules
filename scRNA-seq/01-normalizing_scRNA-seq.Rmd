---
title: "scRNA-seq Normalization"
output:   
  html_notebook: 
    toc: true
    toc_float: true
---

**CCDL 2019**

In this notebook, we'll perform quality control analyses and normalization of 
scRNA-seq count data. 

As opposed to bulk RNA-seq, there are there are a few main things to look out for
in single-cell RNA-seq:

**Single-cell RNA-seq...**

- Requires more PCR amplification (and therefore more PCR-associated biases and
error).  
- Has more zeroes in the gene expression data (most genes aren't expressed 
across cell types).  
People sometimes refer to this as "sparsity" or "dropout."  
  
For this tutorial, we will be using a pair of single-cell analysis specific 
R packages: `scater` and `scran`. 
This tutorial is in part based on the [scran
tutorial.](https://bioconductor.org/packages/devel/bioc/vignettes/scran/inst/doc/scran.html)
  
## Set Up 

For these analyses, we will need `scater` and `scran` packages, which have been 
installed already on your Docker container. 

```{r Set Up}
# Magrittr pipe
`%>%` <- dplyr::`%>%`

# Load the already installed packages
library(scater)
library(scran)
```

## Import single-cell RNA-seq counts matrix

This [data set](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE84465) 
we are using is glioblastoma data that was Fluorescence-Activated Cell sorted 
and then was processed by paired end sequencing using Smart-seq2 protocol 
[(Darmanis et al. _Cell Reports._ 2017.).](https://www.ncbi.nlm.nih.gov/pubmed/29091775).
We're using a subset of this dataset (n = 786 cells).
You have been provided with a counts matrix from single-cell RNA-seq data that 
has already been processed using `Salmon` and `tximport` and saved the counts to 
a tsv file. 
You already know how to process this type of non-tag-based data, because you 
can follow essentially the same steps as we did in the bulk RNA-seq modules.
As far as *tag-based* scRNA-seq data, we will explain how to process raw data, 
like fastq files, in the next section, so you can also get it to a count matrix
and could use these same steps to filter and normalize the data.
  
```{r Import Data}
# Read in the data
sc_data <- readr::read_tsv(file.path("data", "darmanis_counts.tsv"), 
                           progress = FALSE)
```

## Set up a SingleCellExperiment object from count data

```{r Set Up SCE Object}
# Prep a matrix with just the rounded counts so we can create an sce object
counts_mat <- round(as.matrix(sc_data[, -1]))

# Set up the Single Cell Experiment object 
sce_raw <- SingleCellExperiment::SingleCellExperiment(list(counts = counts_mat))
```

Use this space to explore the SingleCellExperiment object you just made.

```{r}
# The SingleCellExperiment is a special type of object used by scater and scran
# R packages. 
# Use what we've learned so far to identify how many genes and cells passed
# the filtering step
```

Let's examine what our filtered counts data looks like with a gene mean density
plot.

```{r}
# Let's calculate the gene means and make a density plot
gene_means <- apply(counts(sce_raw), 1, mean)

# Plot the means
qplot(gene_means, geom = "density", xlab = "counts") + 
  geom_density(fill = "lightblue")
```

The unnormalized, counts data has so many zeroes. 
Actually for this dataset, 78.8% of the data points are zeroes, which is not 
out of the ordinary for single-cell RNA-seq data. 
So we can get a little bit more sensible of a graph, let's plot the same thing
but log transformed.

```{r}
# Plot the log(means)
qplot(log(1 + gene_means), geom = "density", xlab = "counts") + 
  geom_density(fill = "lightblue")
```

As we should expect, our single-cell RNA-seq transcript counts consists of a few
genes that are expressed a lot, and the rest of the genes are barely detected. 

## Normalization of count data 

In whatever data we are working with, we are always looking to maximize biological 
variance and minimize technical variance. 
This is where normalization methods usually come into the workflow.
For our data, many of these zeroes are likely biologically sensible: an individual
cell doesn't need to express every gene.
However, some of these zeroes might be technical in origin: we start with such 
a small amount of RNA in single cell (we are potentially trying to detect just 
a few RNA molecules that might exist for this gene in this cell).  
  
So ideally, we would like to avoid the technical zeroes, but leave the biological 
zeroes alone.
[Lun et al 2016]((https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7)) 
developed a normalization method in `scran` that attempts to do this. 
  
*Overall strategy for `scran::computeSumFactors` based normalization:*
1) Group cells with other like cells by clustering.  
2) Compute sum factors for each cell cluster grouping.  
3) Estimate new counts using linear algebra.  
4) Normalize by the total reads with counts per million reads (CPMs).  
  
Because of differences in sequencing depth, it's pretty common to try to correct
for this by using total per million reads.
You have probably seen data reported that was, in part, normalized based the 
total per million reads for a sample such as counts per million reads (CPMs),
(or TPMs, FPKMs, and RPKMs, which you can [read about more here](https://haroldpimentel.wordpress.com/2014/05/08/what-the-fpkm-a-review-rna-seq-expression-units/) on your own).

#### Normalization step 1: Estimate cell clusters:

```{r}
# Calculate clusters (can adjust min.size depending on clusters sizes you may 
# expect based on prior knowledge)
qclust <- scran::quickCluster(sce_raw, min.size = 100)

# Look at the group sizes before we continue:
summary(qclust)
```

#### Normalization step 2: Compute sum factors:

Computer factors for each sample using the clusters we just made.

```{r}
# Calculate the size factors based on the clusters
sce_raw <- scran::computeSumFactors(sce_raw, clusters = qclust)
```

#### Normalization step 3 and 4: Calculate CPMs and normalize using sum factors:

For more details on the normalization [read here](https://rdrr.io/bioc/scran/man/computeSumFactors.html)

```{r Normalize the Data}
# Normalize the data
sce_norm <- scater::calculateCPM(sce_raw, use_size_factors = TRUE)
```

## Explore and compare normalized data to counts

Let's recreate the plot we did before, except now, let's put our scran 
normalized plotted next to it.

```{r}
# Get means of raw and normalized datasets
all_means <- list("log_counts" = apply(log(1 + counts(sce_raw)), 1, mean), 
                  "log_scran" = apply(log(1 + sce_norm), 1, mean))

# Make into a dataframe so ggplot likes it
all_means <- reshape2::melt(all_means) %>%
  dplyr::mutate("dataset" = factor(L1, levels = c("log_counts", "log_scran"))) %>% 
  dplyr::select(-L1)

# Plot the density of the means 
ggplot(all_means, aes(x = value)) +
  geom_density(fill = "lightblue") +
  facet_wrap(~dataset)
```

The scran normalized data doesn't look quite as 0 heavy as the counts data, 
although we can see it's still clearly skewed to the right. 

## Explore gene variance

Even after normalization, our transcript data is being dominated by a few genes 
that are highly expressed. 
Some of these high expression genes are probably "housekeeping-like" in that 
their expression patterns are stable across different circumstances. 
These "housekeeping-like" genes are unlikely to help us determine biologically 
meaningful gene expression patterns. 
So another way to determine potential genes of interest is to calculate the 
variance.

```{r Calculate Gene Variance}
# Create a gene variance vector
# Here we use the apply function to tell R to peform the function over all the 
# rows (use 1 for rows, 2 for columns)
gene_var <- apply(sce_norm, 1 , sd)

# Create a gene mean vector
gene_mean <- apply(sce_norm, 1 , mean)

# Combine these into a data frame that also contains log(variance/mean)
gene_stats <- data.frame(log_var = log(1 + gene_var), 
                         log_mean = log(1 + gene_mean)) %>% 
  dplyr::mutate(var_mean_ratio = (log_var - log_mean))
```

Let's plot the relationship of gene variance and gene means so we can get a 
better idea of what kinds of genes we are working with.
Here we will plot the `log(variance/mean)` against the `log(mean)` expression 
for each gene.
On your own, it's good practice to see how this graph changes 
with changes of normalization parameters, as well as high variance gene cutoffs.

```{r Plot Variance vs Means}
# Let's plot this with ggplot2
ggplot(gene_stats, aes(x = log_mean, y = var_mean_ratio)) +
  # We are using alpha so that the points are more see-through
  geom_point(alpha = 0.15) +
  xlab("Gene log(Mean)") +
  ylab("Gene log(Variance/Mean)") + 
  cowplot::theme_cowplot() 
```

Because our x axis has the same data that we plotted in the above section, 
you'll notice that we continue to see zero inflation that our single-cell is
known for.

Question: Which the genes plotted here might be of particular interest for our 
further analyses?

## Save the normalized data to tsv file

In case we wanted to return to this data later, let's save the normalized data
to a tsv file. 
`readr` requires that it be a `data.frame` so we will convert our `matrix` first.

```{r Save Data to .tsv}
# Save this gene matrix to a tsv file
sce_norm %>% as.data.frame() %>%
  readr::write_tsv(file.path("data", "normalized_gbm_gene_matrix.tsv"))
```

### Print session info

```{r}
sessionInfo()
```