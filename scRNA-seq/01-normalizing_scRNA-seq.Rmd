---
title: "scRNA-seq Normalization"
output:   
  html_notebook: 
    toc: true
    toc_float: true
---

**CCDL 2019**

In this notebook, we'll perform quality control analyses and normalization of 
scRNA-seq count data. 

As opposed to bulk RNA-seq, there are there are a few main things to look out for
in single-cell RNA-seq:

**Single-cell RNA-seq...**

- Requires more PCR amplification (and therefore more PCR-associated biases and
error).  
- Has more zeroes in the gene expression data (called 'zero inflation').  

Single-cell RNA-seq data zero inflation occurs because of biological and 
technical reasons. 
- _Biologically_, zeroes may arise because a cell doesn't express a gene at a 
particular point in time. 
- Alternatively, zeroes as a result of _technical_ issues, called  gene _dropout_,
are when a cell may have actually expressed a gene but this wasn't detected by 
sequencing. 
  
For this tutorial, we will be using a pair of single-cell analysis specific 
R packages: `scater` and `scran` to work with our data. 
This tutorial is in part based on the [scran
tutorial.](https://bioconductor.org/packages/devel/bioc/vignettes/scran/inst/doc/scran.html)
  
## Set Up 

For these analyses, we will need `scater` and `scran` packages, which have been 
installed already on your Docker container. 

```{r Set Up}
# Read in a function for filtering the gene matrix from a source script
source(file.path("scripts", "GeneMatrixFilter.R"))

# Magrittr pipe
`%>%` <- dplyr::`%>%`

# Load the already installed packages
library(scater)
library(scran)
```

## Import single-cell RNA-seq counts matrix

This [data set](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE84465) 
we are using is glioblastoma data that was Fluorescence-Activated Cell sorted 
and then was processed by paired end sequencing using Smart-seq2 protocol 
[(Darmanis et al. _Cell Reports._ 2017.).](https://www.ncbi.nlm.nih.gov/pubmed/29091775).
We're using a subset of this dataset (n = 1854 cells).
You have been provided with a counts matrix from single-cell RNA-seq data that 
has already been processed using `Salmon` and `tximport` and saved the counts to 
a tsv file. 
You already know how to process this type of non-tag-based data, because you 
can follow essentially the same steps as we did in the bulk RNA-seq modules.
As far as *tag-based* scRNA-seq data, we will explain how to process raw data, 
like fastq files, in the next section, so you can also get it to a count matrix
and could use these same steps to filter and normalize the data.
  
```{r Import Data}
# Read in the data
sc_data <- readr::read_tsv(file.path("data", "unfiltered_darmanis_counts.tsv"), 
                           progress = FALSE)

# Read in the metadata
sc_metadata <- readr::read_tsv(file.path("data", "unfiltered_darmanis_metadata.tsv"), 
                               progress = FALSE)
```

Now that we have loaded our data, let's take a look at what it looks like 
overall. 
To do this, let's calculate the gene means and make a density plot. 

```{r}
# Let's calculate the gene means and make a density plot
gene_means <- apply(sc_data[, -1], 1, mean)

# Plot the means
qplot(gene_means, geom = "density", xlab = "gene mean counts") + 
  geom_density(fill = "lightblue") +
  cowplot::theme_cowplot() 
```

The unnormalized, counts data has so many zeroes, which we should expect. 
For this dataset, 83.3% of the data points are zeroes, which is not 
out of the ordinary for single-cell RNA-seq data. 
This means our data consists of a few genes that are expressed a lot, and the 
rest of the genes are barely detected. 

## Filter the counts matrix
  
Single-cell RNA-seq experiments are generally much larger dimensions than bulk 
RNA-seq, since every individual cell is being measured. 
However, due to the high risk of technical artifacts, we don't necessarily want 
to keep the data for all the cells and genes. 
The small amount of RNA in a single cell results in higher chances of errors or 
biases in amplification steps for some genes. 

#### Total counts as a quality measure

One basic of method of exploring if a particular cell's data is trustworthy is
seeing how many counts were measured total, over all the genes. 
Let's take a look at this data from how many counts are there per sample:
Note that for `apply` the second argument indicates whether the function you 
specify is being applied across all the rows or the columns (1 = rows, 2 = columns).
Also note that in this case we are excluding the `genes` column for the calculation. 

```{r}
# Make a vector of the total number of counts per sample
total_counts <- apply(sc_data[, -1], 2, sum)

# Take a look at the summary statistics for the total counts
summary(total_counts)
```

Yikes, one of the samples has only 48 counts. 
It's highly likely that this 'cell' is either an empty well or the well it was
in didn't get sequenced properly. 
Regardless of what may have happened, we can't trust that sample.
These kinds of things are to be expected; it's highly unlikely that when obtain 
a single-cell RNA-seq dataset that all of cells within the set have reliable, 
high quality data.

This is why exploring your data and filtering it is important before we attempt to
look for any biologically meaningful information. Let's visualize the 
distribution of total counts to see if the 48 count sample is the only one we 
should get rid of.

For the following graphs, we will use vertical red lines to indicate where 
maybe a suggested cutoff should go.

```{r Explore total counts distribution}
# Let's take a look at what this looks like:
hist(total_counts, col = "lightblue", breaks = 40);
  # let's plot what we might want to be our cutoff:
  abline(v = 50000, col = "red")
```

Let's filter out some of the samples that have a lower amount of total counts. 
We also will need to filter the samples in our metadata so it matches our data.

```{r Filter by total counts}
# Filter our data based on this cutoff 
sc_data_filtered <- sc_data %>% 
  tibble::column_to_rownames("genes") %>%
  dplyr::select(which(total_counts > 50000))

# So we can be sure that our data matches or metadata, we will use a filter
sc_metadata_filtered <- sc_metadata %>% 
  dplyr::filter(geo_accession %in% colnames(sc_data_filtered))

# Always good to double check that your data and your metadata matches
all.equal(colnames(sc_data_filtered), sc_metadata_filtered$geo_accession)
```

### Number of genes a cell expresses as a quality measure

What if a particular cell has all its counts belonging to one gene?
This cell would not have helpful data for us, so we should look to remove any 
cells we suspect might not have a useful amount of its transcriptome measured.

But before we can determine how many genes we consider a particular cell to be 
expressing we need to determine a numeric cutoff for what we consider to be a 
detected gene.
How many counts must there be for you to consider a gene expressed? 
For single-cell data, it is probably best to be a bit lenient due to the nature
of the data. 
Here let's go for a simple detection cutoff of > 0 

```{r Create detection matrix}
detection_mat <- as.matrix(sc_data_filtered) > 0 
```

Now that we have turned our data into a matrix of `TRUE/FALSE` for detection, we
can sum this data by column to effectively get a vector of how many genes were 
measured in each cell. 

```{r}
# Make a vector that contains the number of genes expressed by a particular cell
num_genes_exp <- apply(detection_mat, 2, sum)

# Let's take a look at what this looks like:
qplot(num_genes_exp, geom = "density") + 
  geom_density(fill = "lightblue") +
  cowplot::theme_cowplot() 
```

This distribution makes it seem like maybe any cells expressing less than 750
genes might be a good cutoff?

But as you may have noticed in our `sc_metadata` we have cell-type information 
for these data. 
Let's use the power of our single-cell resolution data to our advantage and look 
at this data with cell-type labels. 

```{r}
# Let's match this to the metadata so we can plot samples by their cell type
num_genes_exp_df <- data.frame('geo_accession' = colnames(detection_mat), 
                               num_genes_exp) %>% 
  dplyr::inner_join(sc_metadata, by = 'geo_accession')

# Plot these data
ggplot(num_genes_exp_df, aes(x = num_genes_exp)) + 
  geom_density(fill = "lightblue") +
  geom_vline(xintercept = 750, color = "red") + 
  facet_wrap(~ cell.type.ch1) +
  cowplot::theme_cowplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Although a cutoff of 750 will disproportionally affect the astrocytes and
oligodendrocytes, it looks like this overall will be a decent cutoff. 
It was good for us to map the samples by their cell types so that way we know 
how this may be affecting different cell types differently. 

### Number of samples that express a gene as a quality measure

Now we have an idea of what samples we probably want to get rid of.
But what if our data contains genes that we can't reliably measure in these cells?
Some gene types, like many long non-coding RNAs for example, are very tissue 
specific and wouldn't be useful for us to look at it. 
For example if only one cell is supposedly expressing a particular gene at a low
level, do we think that is biologically relevant, or is it more likely 
_gene dropout_, due to some amplification issue? 
Because our single-cell data is already riddled with zeroes, it's best just to 
get rid of these genes that we suspect are not being reliably measured by many 
cells. 
To do this, we will use our detection matrix to add up how many samples express
each of these genes. 

```{r}
# Make a vector that contains the number of cells that express a particular gene
num_samples_that_exp <- apply(detection_mat, 1, sum)

# Let's take a look at what this looks like:
qplot(num_samples_that_exp, geom = "density") + 
  geom_density(fill = "lightblue") +
  geom_vline(xintercept = 10, col = "red") +
  cowplot::theme_cowplot() 
```

### Apply a filter based on our criteria

Now that we've explored our data, let's used the information we've learned to 
actually filter our data.

```{r Apply filter}
sc_data_filtered <- GeneMatrixFilter(sc_data_filtered, 
                                     min_counts = 0, 
                                     num_samples = 10, 
                                     num_genes = 750)

# Filter metadata accordingly again
sc_metadata_filtered <- sc_metadata_filtered %>% 
  dplyr::filter(geo_accession %in% colnames(sc_data_filtered))
```

Let's see how our filter did in regards to getting rid of samples that are 
likely bad quality according to percent mapped reads data.

```{r Check percent mapped reads}
# Let's read in the percent mapped reads data
perc_map_reads <- readr::read_tsv(file.path("data",
                                            "darmanis_percent_mapped_reads.tsv")) %>%
  # Let's make a new variable that says whether or not the cell was filtered out
  dplyr::mutate("kept" = geo_accession %in% colnames(sc_data_filtered)) 

# Plot the densities of these data
ggplot(perc_map_reads, aes(x = perc_mapped_reads, fill = kept)) +
  geom_density(alpha = 0.15) + 
  colorblindr::scale_fill_OkabeIto() +
  cowplot::theme_cowplot() 
```

The samples we filtered out do appear to include the lower percent mapped reads 
so this is decent support for our filtering methods.

## Set up a SingleCellExperiment object from count data

For the rest of this module and some of the upcoming modules, we are going to 
use some single-cell RNA-seq R packages called `scater` and `scran` to do some 
processing for us.
The functions in these packages require the data to be in their own special 
object type (not an uncommon thing for R packages to do) called `SingleCellExperiment`.
So first, we are going to set up our data in this format.

```{r Set up SCE object}
# Prep a matrix with just the rounded counts so we can create an sce object
counts_mat <- round(as.matrix(sc_data_filtered))

# Set up the Single Cell Experiment object 
sce_raw <- SingleCellExperiment::SingleCellExperiment(list(counts = counts_mat))
```

Special object type are difficult to use at times if we don't know what they 
look like. 
Based on things we've learned in the previous modules, use this space to explore
the `SingleCellExperiment` object you just made.

```{r Explore SCE structure}
# The SingleCellExperiment is a special type of object used by scater and scran
# R packages. 
# Use what we've learned so far to identify how many genes and cells passed
# the filtering step
```

## Normalization of count data 

In whatever data we are working with, we are always looking to maximize biological 
variance and minimize technical variance. 
This is where normalization methods usually come into the workflow.
With this high amount of zeroes in our data, this makes normalization particularly
tricky.
So in order to account for these zeroes, we normalize cells in groups with other 
cells like them; a method introduced in [Lun et al 2016](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7).
  
*Overall strategy for `scran::computeSumFactors` based normalization:*
1) Group cells with other like cells by clustering.  
2) Compute sum factors for each cell cluster grouping.  
3) Normalize using these pooled sum factors and log transform. 

#### Normalization step 1: Estimate cell clusters:

Using a quick clustering, let's determine groupings for our cells that we will
normalize by.
This step may take a minute or so.

```{r}
# Set seed for reproducibility
set.seed(1234)

# Run clustering
qclust <- scran::quickCluster(sce_raw)
```

#### Normalization step 2: Compute sum factors:

Compute factors for each sample using the clusters we just made.
For more details on the `computeSumFactors` step [read here](https://rdrr.io/bioc/scran/man/computeSumFactors.html)

```{r Calculate sum factors}
# Calculate the sum factors based on the clusters
sce_raw <- scran::computeSumFactors(sce_raw, clusters = qclust)
```

The authors of `scran` recommend removing cells that have negative size factors, 
so we will do that here. 
Of course, if you have a lot of cells that turn up has negative size factors, 
you should probably reconsider your filter and/or whether something has affected
the quality of your data. 

#### Normalization step 3: Calculate pooled CPMs and normalize using sum factors:

For more details on the normalization step [read here](https://bioconductor.org/packages/release/bioc/vignettes/scran/inst/doc/scran.html)

```{r Normalize the Data}
# Normalize the data
sce_norm <- scater::normalize(sce_raw)
```

## Compare normalized data to count data

One way to determine whether our normalization is yielding biologically relevant 
results, is to plot it and see if similarly labeled samples and cells end up 
together.
Because plotting 1000's of genes together isn't practical, we will reduce the 
dimensions of our data using Principal Components Analysis. 
Note that the `t()` function is being used here to transpose the `matrix`, so 
it is samples x genes. 
Because, in this instance, we would like to have PCA scores for each cell.
We could however, obtain PCA scores for each gene if we didn't transpose our
data.frame first. 

```{r}
# Use PCA for dimension reduction of cells' raw data
count_pca <- prcomp(t(counts(sce_raw)))

# Use PCA for dimension reduction of cells' scran normalized data
norm_pca <- prcomp(t(logcounts(sce_norm)))
```

There's a lot of data we can pull from these `prcomp` objects, but for now, 
we need the PCA scores for the samples.
In order to retrieve the PCA scores, we will use `$x` at the end of our `prcomp`
objects and because we only really want the PCA scores so we can make our 
scatterplot.

```{r Graph PCA}
# Make this into a dataframe for easy graphing 
pca_scores <- rbind.data.frame(count_pca$x[, 1:2], 
                               norm_pca$x[, 1:2]) %>% 
  # Make data and cell type labels
  dplyr::mutate("data_label" = c(rep("counts", nrow(count_pca$x)),
                                 rep("norm", nrow(norm_pca$x))),
                "cell_type" = rep(sc_metadata_filtered$cell.type.ch1, 2)) %>%
  # Change these into factors
  dplyr::mutate("data_label" = as.factor(data_label),
                "cell_type" = as.factor(cell_type))

# Now plot it
ggplot(pca_scores, aes(x = PC1, y = PC2, color = cell_type)) +
  geom_point() +
  colorblindr::scale_color_OkabeIto() +
  facet_wrap(~ data_label, scales = "free") +
  cowplot::theme_cowplot() 
```

The normalization in particular has appeared to help separate the rest of the 
brain cells from the immune cells. 
Overall, data seems moderately clustered by cell type, and for PC1 in 
particular, this appears to have been improved by the `scran` normalization.

## Save the normalized data to tsv file

In case we wanted to return to this data later, let's save the normalized data
to a tsv file. 
`readr` requires that it be a `data.frame` so we will convert our `matrix` first.

```{r Save Data to .tsv}
# Save this gene matrix to a tsv file
logcounts(sce_norm) %>% as.data.frame() %>%
  readr::write_tsv(file.path("data", "scran_norm_gbm_gene_matrix.tsv"))
```

### Print session info

```{r}
sessionInfo()
```