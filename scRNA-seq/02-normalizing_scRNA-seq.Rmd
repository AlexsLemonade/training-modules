---
title: "scRNA-seq Normalization"
output:   
  html_notebook: 
    toc: true
    toc_float: true
---

**CCDL 2020**

In this notebook, we'll perform normalization of scRNA-seq count data that we have already done quality-control analyses of. 
    
For this tutorial, we will be using a pair of single-cell analysis specific 
R packages: `scater` and `scran` to work with our data. 
This tutorial is in part based on the [scran
tutorial.](https://bioconductor.org/packages/devel/bioc/vignettes/scran/inst/doc/scran.html)
  
## Set Up 

Load the libraries we will be using, and set the random number generation seed value for reproducibility.

```{r setup}
# Set seed for reproducibility
set.seed(1234)

# Magrittr for the pipe %>%
library(magrittr)

# GGPlot2 for the plots
library(ggplot2)

# Packages for single cell processing
library(scater)
library(scran)
```

Now let's set up the files we will be using:

```{r filepaths}
# main data directory
data_dir <- file.path("data", "glioblastoma")

# Filtered count matrix file from previous notebook
filtered_count_file <- file.path(data_dir, "filtered", "filtered_count_matrix.tsv")

# Metadata file location
metadata_file <- file.path(data_dir, "preprocessed", "darmanis_metadata.tsv")

# Output directory
norm_dir <- file.path(data_dir, "normalized")
if (!dir.exists(norm_dir)) {
  dir.create(norm_dir, recursive = TRUE)
}
```


## Read in the filtered count matrix and metadata

```{r read_data}
sc_data_filtered <- readr::read_tsv(filtered_count_file)
sc_metadata <- readr::read_tsv(metadata_file)
```



## Set up a SingleCellExperiment object from count data

Now that we have filtered our data, we are ready to normalize it.
To do this, we are going to use some single-cell RNA-seq R packages called `scater` and `scran`. 

![](diagrams/full-length_2.png)

The functions in these packages require the data to be in their own special object type (not an uncommon thing for R packages to do) called `SingleCellExperiment`.
So first, we are going to set up our data in this format.
To learn more about `SingleCellExperiment` objects and how it works with Bioconductor packages, we recommend this article: [Amezquita et al. 2020.](https://www.nature.com/articles/s41592-019-0654-x).


```{r}
# Set up the Single Cell Experiment object 
gbm_sce <- SingleCellExperiment(list(counts = as.matrix(sc_data_filtered[, -1])))

# Store the gene names in this object
rownames(gbm_sce) <-  sc_data_filtered$gene
colnames(gbm_sce) <- colnames()
```

In the intro-to-R-tidyverse module notebook, `01-intro-to-r`, we discuss base R object types, but there are some 'special' object types that are package-specific. 
`SingleCellExperiment` objects, for example, are what a lot of single-cell analysis R packages use, so we will try to get acquainted with them. 

Use this chunk we've set up below to do some exploring of the `SingleCellExperiment` object you just made.

```{r Explore_SCEstructure, live = TRUE, eval = FALSE}
# The SingleCellExperiment is a special type of object used by scater and scran
# R packages, explore its properties here. 
# Also check it out in the environment tab.
gbm_sce
```

Below is a figure from [Amezquita *et al.* _bioRxiv._ 2019.](https://www.biorxiv.org/content/10.1101/590562v1) (the preprint version of the above-linked paper) that shows the general structure of `SingleCellExperiment` objects. 

![](figures/sce_structure.png)

Note that each function which transforms the `SingleCellExperiment` object can store its results back into the object (or a new copy of the object, but that tends to be a waste of space).
Each of these transformation functions also have a corresponding extraction function that allow you extract (or set) those results from the `SingleCellExperiment` object. 

For more information on `SingleCellExperiment` objects, check out this [excellent vignette](https://bioconductor.org/packages/devel/bioc/vignettes/SingleCellExperiment/inst/doc/intro.html)
and/or [Amezquita et al. _bioRxiv._ 2019.](https://www.biorxiv.org/content/10.1101/590562v1)

### Adding more metadata to the SCE object


```{r sample_info}
# get the sample names and put them in a data frame (1 column)
sample_info <- tibble::tibble(geo_accession = colnames(gbm_sce)) %>%
  #join to the metadata
  dplyr::left_join(sc_metadata) %>%
  dplyr::select(geo_accession,
                title,
                cell_type = cell.type.ch1,
                tissue = characteristics_ch1.3
                ) %>%
  # clean up tissue
  dplyr::mutate(tissue = stringr::str_remove(tissue, "tissue: "))
```

Check that the sample info accession ids are the same as the columns of our data

```{r check_sampleinfo, live = TRUE}
all.equal(sample_info$geo_accession, colnames(gbm_sce))
```

Now add that data to the SCE object.
For unclear reasons, we have to convert our table to a `DataFrame` object in order for this to work.
Just to keep things confusing, a `DataFrame` is not the same as a `data.frame` that we have been using throughout. 
We also need to be sure to include the `row.names` argument to keep those properly attached

```{r}
colData(gbm_sce) <- DataFrame(sample_info, row.names = sample_info$geo_accession)
```


## Normalization of count data 

In whatever data we are working with, we are always looking to maximize biological variance and minimize technical variance. 
A primary source of technical variation we are concerned with is the variation in library sizes among our samples. 
While different cells may have different total transcript counts, it seems far more likely that the primary source of variation that we see is due to library construction, amplification, and sequencing. 

This is where normalization methods usually come into the workflow.
The distribution of the counts that we saw in the previous notebook, and in particular the fact that the count data is noisy with many zero counts, makes normalization particularly tricky.
To handle this noise, we normalize cells in groups with other cells like them; a method introduced in [Lun *et al.* (2016)](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7).

Briefly, we first cluster the cells to find groups of similar cells, then compute normalization factors based on the sums of expression in those groups. 
The group normalization is then applied back to the individual cells within the group to create a normalized count matrix. 
In this case, we will also log-transform the normalized counts to get a less skewed distribution of expression measures. 
Note that because of the zero counts, the logNormCounts() function will add a pseudocount of 1 to each value before computing the log.

*These steps may take a minute or so.*

```{r sce_normalize}
# Step 1) Group cells with other like cells by clustering.  
qclust <- scran::quickCluster(gbm_sce)

# Step 2) Compute sum factors for each cell cluster grouping.  
gbm_sce <- scran::computeSumFactors(gbm_sce, clusters = qclust)

# Step 3) Normalize using these pooled sum factors and log transform. 
gbm_sce <- scater::logNormCounts(gbm_sce)
```

## Compare normalized data to count data

One way to determine whether our normalization yields biologically relevant results is to plot it and see if similarly labeled samples and cells end up together.
Because plotting 1000's of genes together isn't practical, we will reduce the dimensions of our data using Principal Components Analysis. 

Note that the `t()` function is being used here to transpose the count matrix, so it is samples x genes, resulting in PCA scores calculated with respect to the samples. 
We could however, obtain PCA scores for each gene if we didn't transpose our matrix first. 

We will also log transform the raw counts to make their scaling more comparable to the normalized data. 
To do this we will use the `log1p()` function, which is specifically designed for the case where we want to add 1 to all of our values before taking the log, as we do here.
(We could do something like `log(counts + 1)`, but this is both more efficient and more accurate.)

```{r pca}
# PCA on the raw counts, log transformed
log_pca <- t(counts(gbm_sce)) %>% # transpose the count matrix
  log1p() %>% # log transform to make these comparable
  prcomp() # calculate PCA scores

# Use PCA for dimension reduction of cells' scran normalized data
norm_pca <- t(logcounts(gbm_sce)) %>%
  prcomp()
  
```

Now we will retrieve the PC scores, by using `$x` at the end of the objects created by `prcomp`.

```{r pca_df}
# Make the first two PC scores from the "raw" data into a dataframe 
# with cell type labels for graphing 
log_pca_scores <- data.frame(log_pca$x[, 1:2],
                            geo_accession = rownames(log_pca$x),
                            cell_type = gbm_sce$cell_type)

# Set up the normalized PC scores in the same way
norm_pca_scores <- data.frame(norm_pca$x[, 1:2],
                              geo_accession = rownames(norm_pca$x),
                              cell_type = gbm_sce$cell_type)
```

Let's plot the unnormalized PCA scores with their cell labels:

```{r pca_plot}
# Now plot counts pca
ggplot(log_pca_scores, aes(x = PC1, y = PC2, color = cell_type)) +
  geom_point() + 
  ggtitle("Log Counts PCA Scores") + 
  colorblindr::scale_color_OkabeIto() # We are adding this so it is colorblind friendly
```

We've plotted the counts data for you. Knowing that we want the same graph, but different data, use the above template to plot the normalized data. 
Feel free to customize the plot with a different theme or color scheme!

Let's plot the `norm_pca_scores` data:

```{r norm_pca_plot, live = TRUE}
ggplot(norm_pca_scores, aes(x = PC1, y = PC2, color = cell_type)) +
  geom_point() + 
  ggtitle("Normalized log counts PCA Scores") + 
  colorblindr::scale_color_OkabeIto()
```

Do you see an effect from the normalization in the comparison between these plots?

## Modeling variance

The variation in gene expression we see among cells comes from a combination of variation due to technical effects and the biology we really care about. 

```{r model_variance}
dec <- modelGeneVar(gbm_sce)
```

```{r}
ggplot(as.data.frame(dec), aes(x= mean, y = total)) +
  geom_point(alpha = 0.05) +
  stat_function(fun = metadata(dec)$trend, color = "blue") + 
  xlab("Mean log-expression") + 
  ylab("Variance")

```

```{r}
highvar_genes <- getTopHVGs(dec, prop=0.1)
subset_pca <- t(logcounts(gbm_sce)[highvar_genes,]) %>%
  prcomp()
```

```{r}
subset_pca_scores <- data.frame(subset_pca$x[, 1:2],
                              geo_accession = rownames(subset_pca$x),
                              cell_type = gbm_sce$cell_type)
```

```{r}
ggplot(subset_pca_scores, aes(x = PC1, y = PC2, color = cell_type)) +
  geom_point() + 
  ggtitle("Normalized log counts PCA Scores") + 
  colorblindr::scale_color_OkabeIto()
```


## Save the normalized data to tsv file

In case we wanted to return to this data later, let's save the normalized data
to a tsv file. 
In order to do this we need to extract our normalized counts from `gbm_sce`. 
Refer back to the `SingleCellExperiment` figure above to determine why we are using this `logcounts` function.

```{r Save Data to .tsv}
# Save this gene matrix to a tsv file
logcounts(gbm_sce) %>% 
  as.data.frame() %>%
  readr::write_tsv(file.path(norm_dir, "scran_norm_gene_matrix.tsv"))
```

Recall that `readr::write_tsv` requires a data frame; so we need to convert 
our `matrix` to a data frame.

We will return to our normalized `gbm_sce` object in the exercise, so we will 
also save our data in an RDS file so that we can re-load it into our R 
environment as a `SingleCellExperiment` object.

```{r}
# Save the data as an RDS
readr::write_rds(gbm_sce, file.path(norm_dir, "glioblastoma_sce.RDS"))
```

### Print session info

```{r}
sessionInfo()
```
