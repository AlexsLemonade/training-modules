---
title: "Dimension Reduction with scRNA-seq data"
author: CCDL for ALSF
date: 2021
output:
  html_notebook: 
    toc: true
    toc_float: true
---

## Objectives

This notebook will demonstrate how to:

- Read Cell Ranger data into R 
- Apply dimensionality reduction methods to Single Cell Data
- Visualize reduced dimensionality data

---

In this notebook, we'll try out some dimension reduction techniques on single-cell RNA-seq data. 

Visualizing highly dimensional data is a common challenge in genomics, and especially with RNA-seq data.
The expression of every gene we look at is another dimension describing a sample.
When we also have hundreds or thousands of individual samples, as in the case of single-cell analysis, figuring out how to clearly display all of the data in a meaningful way is difficult. 

A common practice is to common to use dimension reduction techniques so all of the data is in a more manageable form for plotting, clustering, and other downstream analyses. 



## Set Up 

```{r setup}
# Load libraries
library(ggplot2)
library(scater)
library(scran)

# Magrittr pipe
library(magrittr)

# Setting the seed for reproducibility
set.seed(12345)
```



## Directories and files

The data we will be using for this module comes from a a 10X Genomics data set of expression data from a  Hodgkin's Lymphoma tumor. 
The data was generated with the 10Xv3.1 chemistry, and processed with Cell Ranger and 10X Genomics standard pipeline.
https://support.10xgenomics.com/single-cell-gene-expression/datasets/4.0.0/Parent_NGSC3_DI_HodgkinsLymphoma

There are a variety of files that you will often see as part of the standard output from Cell Ranger, which are described in detail in [10X Genomics documentation](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/overview).
We have included some of these in the `data/hodkins/cellranger` directory, including the `web_summary.html` file that includes some similar QC statistics to those we generated with `alevinQC`.
The main file we will be working with are the feature by barcode matrices.
Cell Ranger does some filtering on its own, but we will start with the raw data.



```{r filepaths}
# main data directory
data_dir <- file.path("data", "hodgkins")

# Path to the Cell Ranger matrix
raw_matrix_dir <- file.path(data_dir, "cellranger", 
                            "raw_feature_bc_matrix")

# Path to mitochondrial genes table
mito_file <- file.path(data_dir, "hs_mitochondrial_genes.tsv")

# Directory and file to save output
normalized_dir <- file.path(data_dir, "normalized")
if (!dir.exists(normalized_dir)) {
  dir.create(normalized_dir, recursive = TRUE)
}

output_sce_file <- file.path(normalized_dir, "normalized_hodgkins_sce.rds")

```

## Reading Cell Ranger data

Cell Ranger output includes count data in two main formats.
The first is a folder with a feature list, a barcode list, and a sparse matrix in ["Matrix Exchange" format](https://math.nist.gov/MatrixMarket/formats.html). 
The `DropletUtils::read10xCounts()` function takes this directory and reads in the data from these three files, assembling the `SingleCellExperiment` object we have worked with before. 

Alternatively, we could use the HDF5 format file that Cellranger outputs as a file with the `.h5` extension, which contains the same data. 
For whatever reason, the way you read the data affects how it is stored in R, with the net result that reading from the directory results in smaller objects in R, so that is what we will do here. 

Cell Ranger also outputs both filtered and raw matrices; today we will start with the raw matrix and perform our own filtering.

```{r}
hodgkins_sce <- DropletUtils::read10xCounts(raw_matrix_dir)
```

How many potential cells are there here?

```{r}
dim(hodgkins_sce)
```

That is a lot of cells! 
In fact, it is really every possible barcode, whether there were reads associated with it or not.
We should probably do something about that.


## QC and normalization

### Basic QC stats

We will start by calculating the basic QC stats as we have done previously, adding those to our `SingleCellExperiment` object.

The first step again is reading in our table of mitochrondrial genes and finding the ones that were quantified our data set.

```{r mitogenes}
mito_genes <- readr::read_tsv(mito_file) %>%
  dplyr::filter(gene_id %in% rownames(hodgkins_sce)) %>%
  dplyr::pull(gene_id)
```

Next we will calculate the QC stats that we used before.
Note that this is much slower than before, as we have many more genes in the unfiltered set!

```{r calculateQC}
hodgkins_sce <- scater::addPerCellQC(
  hodgkins_sce, 
  subsets = list(mito = mito_genes))
```

We can now do the most basic level of filtering: getting rid of "cells" with no reads.

```{r remove_zero}
hodgkins_sce <- hodgkins_sce[, hodgkins_sce$total > 0]
dim(hodgkins_sce)
```

### Filtering with `emptyDrops()`

The `DropletUtils` package that we used to read in the 10X data has a number of other useful features. 
One is the `emptyDrops()` function, which uses the overall expression patterns in the sample to identify droplets that are likely to not contain an intact cell, but may simply have contained loose ambient RNA released during cell separation. 
This method, described in [Lun *et al.* (2019)](https://doi.org/10.1186/s13059-019-1662-y), uses the droplets with very low UMI counts to estimate the "ambient" expression pattern, then scores the remaining cells based how much they deviate from that pattern.
This method seems to perform well to exclude false "cells" while retaining cells with distinct expression profiles but low counts that might have failed a simple cutoff. 

The `emptyDrops()` function takes the `counts` matrix from our SingleCellExperiment, and returns a data frame with the statistics it calculated.
This will take a few minutes to run, but we can speed it up by allowing parallel processing.


```{r emptydrops}
droplet_stats <- DropletUtils::emptyDrops(
  counts(hodgkins_sce),
  # use multiprocessing
  BPPARAM = BiocParallel::MulticoreParam(4))
```


We will use a false discovery rate (FDR) of 0.01 as our cutoff for "real" cells. 
Since `emptyDrops()` uses low count cells to estimate the "ambient" expression pattern, those cells are not assigned an FDR value, and have a value of NA.
These NAs can be a problem for filtering with a Boolean vector, as we did above, so instead we will use the `which()` function to get the *positions* of the cells that pass our filter and select the columns we want using that.

```{r filter_empty, live = TRUE}
cells_to_retain <- which(droplet_stats$FDR <= 0.01)

filtered_sce <- hodgkins_sce[, cells_to_retain]
dim(filtered_sce)
```

How does this compare to the number of cells in the Cell Ranger filtered data? 
Looking the `web_summary.html` report from Cell Ranger, it seems that it would have kept 3,394 cells, so we seem to be getting broadly similar results.

### Checking mitochondrial content

While `emptyDrops()` should have filtered out droplets containing no cells, it will not necessarily filter out damaged cells.
For that we will still want to look at mitochondrial content, as we did previously.  
The statistics we calculated earlier with `addPerCellQC()` are retained in our new object, so we can plot those directly.

```{r mito_percent_plot, live = TRUE}
# Plot the mitochondrial percents stored in `filtered_sce`
ggplot( mapping = aes(x = filtered_sce$subsets_mito_percent)) +
  geom_histogram(bins = 100) 
```
There are certainly some cells with high mitochondrial percentages!
For now, we will use a cutoff of 20% to filter out the worst of the cells.

```{r}
filtered_sce <- filtered_sce[, filtered_sce$subsets_mito_percent < 20]
```


We can also filter by features (genes in our case) using `scater::addPerFeatureQC()` which will computer the number of samples where each gene is detected and the mean count across all genes. 
We can then use those data (stored in `rowData`) to filter by row to only the genes that are detected in at least 5% of cells, and with a mean count > 0.1.

```{r gene_qc}
filtered_sce <- scater::addPerFeatureQC(filtered_sce)
detected <- rowData(filtered_sce)$detected > 5
expressed <- rowData(filtered_sce)$mean > 0.1

# filter the genes (rows) this time
filtered_sce <- filtered_sce[detected & expressed, ]
```

```{r}
dim(filtered_sce)
```


### Normalize

Now we will perform the same normalization steps we did with a previous dataset, using `scran::computeSumFactors()` and `scater::logNormCounts()`

```{r normalize}
# Cluster similar cells
qclust <- scran::quickCluster(filtered_sce)

# Compute sum factors for each cell cluster grouping.  
filtered_sce <- scran::computeSumFactors(filtered_sce, clusters = qclust)

# Normalize and log transform. 
filtered_sce <- scater::logNormCounts(filtered_sce)
```


![**Status of the current dataset**](diagrams/tag-based_3.png)

## Dimensionality reduction and display

### Principal Components Analysis

The first dimensionality reduction algorithm we will use is PCA, which we have already used!
The idea of PCA is that it finds the axes which explain the greatest amount of variance in the data.


```{r PCA}
# Run PCA
hodgkins_pca <- calculatePCA(filtered_sce)
```

```{r plot_pca}
# Plot this with ggplot2 
ggplot(data.frame(hodgkins_pca), 
       aes(x = PC1, y = PC2)) + 
  geom_point(alpha = 0.2) +
  theme_bw()
```
It looks like there may be a few clusters there, though they are not very distinct.
Sometimes looking at higher dimensions can show more (or different) structure, so lets have a look there too.

```{r plot_pca2}
# Plot this with ggplot2 
ggplot(data.frame(hodgkins_pca), 
       aes(x = PC3, y = PC4)) + 
  geom_point(alpha = 0.2) +
  theme_bw()
```

### Storing PCA results with the data

Let's store the PCA results in our `SingleCellExperiment` object, as we might want to use them later.
To do this, we will use the `runPCA()` function from scater, which performs the same calculation as above, but returns a new object with the results stored in the `reducedDim` slot.

```{r}
filtered_sce <- runPCA(filtered_sce)
```

We can see what reduced dimensionality matrixes are stored in the object with `reducedDimNames()`.

```{r}
reducedDimNames(filtered_sce)
```

To extract them by name, we use the `reducedDim()` function, much like the `assay()` function to extract original data.

```{r}
reducedDim(filtered_sce, "PCA")[1:10, 1:5]
```



### UMAP

**UMAP** (Uniform Manifold Approximation and Projection) is a machine learning technique designed to provide more detail in highly dimensional data than a typical principal components analysis. 
While PCA assumes that the variation we care about has a particular distribution (normal, broadly speaking), UMAP allows more complicated distributions that it learns from the data. 
The underlying mathematics are beyond me, but if you are more ambitious than I, you can look at the paper by [McInnes, Healy, & Melville (2018)](https://arxiv.org/abs/1802.03426). 
The main advantage of this change in underlying assumptions is that UMAP can do a better job separating clusters, especially when some of those clusters may be more similar to each other than others.  

Another dimensionality reduction technique that you may have heard of is **t-SNE** (t-distributed Stochastic Neighbor Embedding), which has similar properties to UMAP, and often produces similar results. 
There is some ongoing debate about which of these two techniques is superior, and whether the differences are due to the underlying algorithm or to implementation and parameter initialization defaults. 
Regardless of why, in our experience, UMAP seems to produce slightly better results and run a bit faster, but the differences can be subtle.


### Default parameters

For ease of use with this data, we will be using the `scater::calculateUMAP()` and `scater::runUMAP()` function to apply UMAP to our single cell data, but similar functions the `uwot` package (notably `uwot::umap()`) can be used to apply UMAP to any numerical matrix.

UMAP can be slow for a large data set with lots of parameters.
It is worth noting that the `scater::calculateUMAP()` implementation actually does PCA first, and then runs UMAP on the top 50 PCs. 

`scater::calculateUMAP()` will return a matrix of results, with one row for each sample, and a column for each of the UMAP dimensions returned. 
As with PCA, `runUMAP()` performs the same function, but stores the results in a SingleCellExperiment object.

Let's see how it looks with the default parameters:

```{r calculate_umap, live = TRUE}
# Run umap
filtered_sce <- runUMAP(filtered_sce)
```

If we have the UMAP results stored in the `SingleCellExperiment` object, we can use the `scater::plotReducedDim()` function to plot it a bit more easily!
This function still uses `ggplot2`, so if we wanted to customize it later, we could.

```{r plot_umap, live = TRUE}
# make a UMAP plot with `plotReducedDim()`
plotReducedDim(filtered_sce, "UMAP", colour_by = "subsets_mito_percent")

```

There is clearly a lot of structure in there, but is it meaningful?
Do the clusters we see differentiate cell types? How should we divide them up?

We will come back to this question later!


### t-SNE comparison

In the block below is a similar analysis and plot with t-SNE (t-distributed Stochastic Neighbor Embedding).
Note that this analysis also does PCA before moving on to the fancy machine learning.

```{r tsne}
# Run TSNE
filtered_sce <- runTSNE(filtered_sce)

# plot with builtin function
plotReducedDim(filtered_sce, "TSNE", colour_by = "subsets_mito_percent")
```

Different! (Slower!) Is it better or worse? Hard to say!
Different people like different things, and one plot might illustrate a particular point better than another. 

## Save results

We are going to use this data more in the next notebook, so let's save it as an `RDS` file.

```{r}
readr::write_rds(filtered_sce, file = output_sce_file)
```


### Some further reading on dimension reduction:  

- This website explains [PCA visually](http://setosa.io/ev/principal-component-analysis/).  
- [Becht *et al.* (2018)](https://www.nature.com/articles/nbt.4314) discusses using [UMAP](https://github.com/lmcinnes/umap) for single-cell data.  
- [Wattenberg *et al.* (2016)](https://distill.pub/2016/misread-tsne/) discuss how to use t-SNE properly with great visuals. 
(The lessons apply to UMAP as well, with a broad substitution of the `n_neighbors` parameter for `perplexity`.)
- [Nguyen & Holmes (2019)](https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1006907&type=printable) lay out guidelines on choosing dimensions reduction methods.  
- [Freitag (2019)](https://rpubs.com/Saskia/520216) is a nice explanation and comparison of many different dimensionality reduction techniques that you may encounter.


## Session Info:

```{r}
sessionInfo()
```
