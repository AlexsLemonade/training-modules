---
title: "scRNA-seq Dimension Reduction"
output:   
  html_notebook: 
    toc: true
    toc_float: true
---

**CCDL 2020**

In this notebook, we'll try out some dimension reduction techniques on single-cell RNA-seq data. 

Visualizing highly dimensional data is a common challenge in genomics, and especially with RNA-seq data.
The expression of every gene we look at is another dimension describing a sample.
When we also have hundreds or thousands of individual samples, as in the case of single-cell analysis, figuring out how to clearly display all of the data in a meaningful way is difficult. 

A common practice is to common to use dimension reduction techniques so all of the data is in a more manageable form for plotting, clustering, and other downstream analyses. 



## Set Up 

```{r setup}
# Load libraries
library(ggplot2)
library(scater)
library(scran)

# Magrittr pipe
library(magrittr)

# Setting the seed for reproducibility
set.seed(12345)
```



## Directories and files

The data we will be using for this module comes from a a 10X Genomics data set of expression data from a  Hodgkin's Lymphoma tumor. 
The data was generated with the 10Xv3.1 chemistry, and processed with Cell Ranger and 10X Genomics standard pipeline.
https://support.10xgenomics.com/single-cell-gene-expression/datasets/4.0.0/Parent_NGSC3_DI_HodgkinsLymphoma

There are a variety of files that you will often see as part of the standard output from Cell Ranger, which are described in detail in [10X Genomics documentation](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/output/overview).
We have included some of these in the `data/hodkins/cellranger` directory, including the `web_summary.html` file that includes some similar QC statistics to those we generated with `alevinQC`.
The main file we will be working with are the feature by barcode matrices.
Cell Ranger does some filtering on its own, but we will start with the raw data.



```{r filepaths}
# main data directory
data_dir <- file.path("data", "hodgkins")

# Path to the Cell Ranger matrix
raw_matrix_dir <- file.path(data_dir, "cellranger", 
                            "raw_feature_bc_matrix")

# Path to mitochondrial genes table
mito_file <- file.path(data_dir, "hs_mitochondrial_genes.tsv")
```

## Reading Cell Ranger data

Cell Ranger output includes count data in two main formats.
The first is a folder with a feature list, a barcode list, and a sparse matrix in ["Matrix Exchange" format](https://math.nist.gov/MatrixMarket/formats.html). 
The `DropletUtils::read10xCounts()` function takes this directory and reads in the data from these three files, assembling the `SingleCellExperiment` object we have worked with before. 

Alternatively, we could use the HDF5 format file that Cellranger outputs as a file with the `.h5` extension, which contains the same data. 
For whatever reason, the way you read the data affects how it is stored in R, with the net result that reading from the directory results in smaller objects in R, so that is what we will do here. 

Cell Ranger also outputs both filtered and raw matrices; today we will start with the raw matrix and perform our own filtering.

```{r}
hodgkins_sce <- DropletUtils::read10xCounts(raw_matrix_dir)
```

How many potential cells are there here?

```{r}
dim(hodgkins_sce)
```

That is a lot of cells! 
In fact, it is really every possible barcode, whether there were reads associated with it or not.
We should probably do something about that.


## QC and normalization

### Basic QC stats

We will start by calculating the basic QC stats as we have done previously, adding those to our `SingleCellExperiment` object.

The first step again is reading in our table of mitochrondrial genes and finding the ones that were quantified our data set.

```{r mitogenes}
mito_genes <- readr::read_tsv(mito_file) %>%
  dplyr::filter(gene_id %in% rownames(hodgkins_sce)) %>%
  dplyr::pull(gene_id)
```

Next we will calculate the QC stats that we used before.
Note that this is much slower than before, as we have many more genes in the unfiltered set!

```{r calculateQC}
hodgkins_sce <- scater::addPerCellQC(
  hodgkins_sce, 
  subsets = list(mito = mito_genes))
```

We can now do the most basic level of filtering: getting rid of "cells" with no reads.

```{r remove_zero}
hodgkins_sce <- hodgkins_sce[, hodgkins_sce$total > 0]
dim(hodgkins_sce)
```

### Filtering with `emptyDrops()`

The `DropletUtils` package that we used to read in the 10X data has a number of other useful features. 
One is the `emptyDrops()` function, which uses the overall expression patterns in the sample to identify droplets that are likely to not contain an intact cell, but may simply have contained loose ambient RNA released during cell separation. 
This method, described in [Lun *et al.* (2019)](https://doi.org/10.1186/s13059-019-1662-y), uses the droplets with very low UMI counts to estimate the "ambient" expression pattern, then scores the remaining cells based how much they deviate from that pattern.
This method seems to perform well to exclude false "cells" while retaining cells with distinct expression profiles but low counts that might have failed a simple cutoff. 

The `emptyDrops()` function takes the `counts` matrix from our SingleCellExperiment, and returns a data frame with the statistics it calculated.
This will take a few minutes to run, but we can speed it up by allowing parallel processing.


```{r emptydrops}
droplet_stats <- DropletUtils::emptyDrops(
  counts(hodgkins_sce),
  # use multiprocessing
  BPPARAM = BiocParallel::MulticoreParam(4))
```


We will use a false discovery rate (FDR) of 0.01 as our cutoff for "real" cells. 
Since `emptyDrops()` uses low count cells to estimate the "ambient" expression pattern, those cells are not assigned an FDR value, and have a value of NA.
These NAs can be a problem for filtering with a Boolean vector, as we did above, so instead we will use the `which()` function to get the *positions* of the cells that pass our filter and select the columns we want using that.

```{r filter}
cells_to_retain <- which(droplet_stats$FDR <= 0.01)

filtered_sce <- hodgkins_sce[, cells_to_retain]
dim(filtered_sce)
```

How does this compare to the number of cells in the Cell Ranger filtered data? 
Looking the `web_summary.html` report from Cell Ranger, it seems that it would have kept 3,394 cells, so we seem to be getting broadly similar results.

### Checking mitochondrial content

While `emptyDrops()` should have filtered out droplets containing no cells, it will not necessarily filter out damaged cells.
For that we will still want to look at mitochondrial content, as we did previously.  
The statistics we calculated earlier with `addPerCellQC()` are retained in our new object, so we can plot those directly.

```{r}
cell_qc_df <- as.data.frame(colData(filtered_sce)) 

# Plot some aspect of the QC data stored in `cell_qc_df`
ggplot( mapping = aes(x = filtered_sce$subsets_mito_percent)) +
  geom_histogram(bins = 100) 
```
There are certainly some cells with high mitochondrial percentages!
For now, we will use an arbitrary cutoff of 50% to filter out the worst of the cells.

```{r}
filtered_sce <- filtered_sce[, filtered_sce$subsets_mito_percent < 50]
```


We can also filter by features (genes in our case) using `scater::addPerFeatureQC()` which will computer the number of samples where each gene is detected and the mean count across all genes. 
We can then use those data (stored in `rowData`) to filter by row to only the genes that are detected in at least 5% of cells, and with a mean count > 0.1.

```{r gene_qc}
filtered_sce <- scater::addPerFeatureQC(filtered_sce)
detected <- rowData(filtered_sce)$detected > 5
expressed <- rowData(filtered_sce)$mean > 0.1

# filter the genes (rows) this time
filtered_sce <- filtered_sce[detected & expressed, ]
```

```{r}
dim(filtered_sce)
```


### Normalize

Now we will perform the same normalization steps we did with a previous dataset, using `scran::computeSumFactors()` and `scater::logNormCounts()`

```{r normalize}
# Cluster similar cells
qclust <- scran::quickCluster(filtered_sce)

# Compute sum factors for each cell cluster grouping.  
filtered_sce <- scran::computeSumFactors(filtered_sce, clusters = qclust)

# Normalize and log transform. 
filtered_sce <- scater::logNormCounts(filtered_sce)
```


![**Status of the current dataset**](diagrams/tag-based_3.png)

## Dimensionality reduction and display

### Principal Components Analysis

The first dimensionality reduction algorithm we will use is PCA, which we have already used!
The idea of PCA is that it finds the axes which explain the greatest amount of variance in the data.


```{r PCA}
# Run PCA
hodgkins_pca <- calculatePCA(filtered_sce)
```

```{r plot_pca}
# Plot this with ggplot2 
ggplot(data.frame(hodgkins_pca), 
       aes(x = PC1, y = PC2)) + 
  geom_point(alpha = 0.2) +
  theme_bw()
```
It looks like there may be a few clusters there, though they are not very distinct.
Sometimes looking at higher dimensions can show more (or different) structure, so lets have a look there too.

```{r plot_pca2}
# Plot this with ggplot2 
ggplot(data.frame(hodgkins_pca), 
       aes(x = PC3, y = PC4)) + 
  geom_point(alpha = 0.2) +
  theme_bw()
```

### Storing PCA results with the data

Let's store the PCA results in our `SingleCellExperiment` object, as we might want to use them later.
To do this, we will use the `runPCA()` function from scater, which performs the same calculation as above, but returns a new object with the results stored in the `reducedDim` slot.

```{r}
filtered_sce <- runPCA(filtered_sce)
```

We can see what reduced dimensionality matrixes are stored in the object with `reducedDimNames()`.

```{r}
reducedDimNames(filtered_sce)
```

To extract them by name, we use the `reducedDim()` function, much like the `assay()` function to extract original data.

```{r}
reducedDim(filtered_sce, "PCA")[1:10, 1:5]
```



### UMAP

**UMAP** (Uniform Manifold Approximation and Projection) is a machine learning technique designed to provide more detail in highly dimensional data than a typical principal components analysis. 
While PCA assumes that the variation we care about has a particular distribution (normal, broadly speaking), UMAP allows more complicated distributions that it learns from the data. 
The underlying mathematics are beyond me, but if you are more ambitious than I, you can look at the paper by [McInnes, Healy, & Melville (2018)](https://arxiv.org/abs/1802.03426). 
The main advantage of this change in underlying assumptions is that UMAP can do a better job separating clusters, especially when some of those clusters may be more similar to each other than others.  

Another dimensionality reduction technique that you may have heard of is **t-SNE** (t-distributed Stochastic Neighbor Embedding), which has similar properties to UMAP, and often produces similar results. 
There is some ongoing debate about which of these two techniques is superior, and whether the differences are due to the underlying algorithm or to implementation and parameter initialization defaults. 
Regardless of why, in our experience, UMAP seems to produce slightly better results and run a bit faster, but the differences can be subtle.


### Default parameters

For ease of use with this data, we will be using the `scater::calculateUMAP()` and `scater::runUMAP()` function to apply UMAP to our single cell data, but similar functions the `uwot` package (notably `uwot::umap()`) can be used to apply UMAP to any numerical matrix.

UMAP can be slow for a large data set with lots of parameters.
It is worth noting that the `scater::calculateUMAP()` implementation actually does PCA first, and then runs UMAP on the top 50 PCs. 

`scater::calculateUMAP()` will return a matrix of results, with one row for each sample, and a column for each of the UMAP dimensions returned. 
As with PCA, `runUMAP()` performs the same function, but stores the results in a SingleCellExperiment object.

Let's see how it looks with the default parameters:

```{r calculate_umap}
# Run umap
filtered_sce <- runUMAP(filtered_sce)


hodgkins_umap <- reducedDim(filtered_sce, "UMAP") %>%
  # Convert to a data frame for plotting
  data.frame() %>%
  # rename for interpretability
  dplyr::rename(UMAP1 = "X1", UMAP2 = "X2") %>%
  # add on some metadata for each row
  dplyr::mutate(mito_percent = filtered_sce$subsets_mito_percent)

```


```{r plot_umap}

# Plot this with ggplot2 
ggplot(hodgkins_umap, 
       aes(x = UMAP1, y = UMAP2, color = mito_percent)) + 
  geom_point(alpha = 0.1) +
  scale_color_viridis_c() + 
  theme_bw()
```

There is clearly a lot of structure in there, but is it meaningful?

Do the clusters we see differentiate cell types? How should we divide them up?

## Clustering cells


```{r}
nn_graph <- scran::buildSNNGraph(filtered_sce, k=20, use.dimred = 'PCA')
clusters <- igraph::cluster_walktrap(nn_graph)$membership
colLabels(filtered_sce) <- factor(clusters)
```

```{r}
plotReducedDim(filtered_sce, "UMAP", colour_by = "label")
```






## UMAP experiments

Now that we have an idea of what a UMAP plot with the default parameters looks like, let's try experimenting with the `n_neighbors` parameter. 
First, we should see what this parameter is, and what the default value is.
In the console, run `?scater::calculateUMAP` to see what this (and other parameters) are.
For even more parameters, you can look at the underlying implementation code that `calculateUMAP()` uses, which is the function `uwot::umap()`

In order to make our experimentation easier, we will create a *function* that allows us to rerun the same code chunk easily, but create an argument that allows us to change one variable: the n_neighbors variable. 

```{r UMAP-function}
UMAP_plot_wrapper <- function(sce = filtered_sce, nn_param = 15) {
  # Purpose: Run UMAP and plot the output
  # Args: nn_param: a single numeric argument that will change the 
  #                 n_neighbors variable in the calculateUMAP() function. 
  # Output: a ggplot scatterplot with the two UMAP coordinates plotted

  # Run UMAP with a specified n_neighbors parameter
  
  umap_mat <- scater::calculateUMAP(sce, n_neighbors = nn_param)
  
  # Plot this with ggplot2
  ggplot(data.frame(umap_mat, label = sce$label), 
         aes(x = X1, y = X2, color = label) )+ 
    geom_point(alpha = 0.2) +
    xlab("UMAP1") + 
    ylab("UMAP2") + 
    theme_classic()
}
```

Let's make sure that works and gives the same result as before when we use the default parameters.

```{r function-test}
UMAP_plot_wrapper(nn_param = 15)
```

*Kind of?*

This isn't your fault! 
UMAP is a non-deterministic function, which means that there is a random component to the results. 
We can use `set.seed()` to be sure that an individual run (or set of runs) is the same every time you run your analysis, but it is important to check your results a few times with different random starting points to be sure that the random component is not giving you anomalous results.
Setting a different random number seed with `set.seed()` is one way to do this, or you can run the analysis multiple times in the same session, as we have done here.

Fill in the next few code chunks with the function and the `n_neighbors` argument you would like to use for each. 
(Feel free to add more tests!)
Then run the chunks and compare your output graphs.

```{r run-UMAP-1, live = TRUE}
# Try something low?
UMAP_plot_wrapper(nn_param = 3)
```

```{r run-UMAP-2, live = TRUE}
# Try something high?
UMAP_plot_wrapper(nn_param = 100)
```

```{r run-UMAP-3, live = TRUE}
# Try whatever you like!
UMAP_plot_wrapper(nn_param = 5)
```

#### Some 'big picture' thoughts to take from this experiment: 

1. Analyses (such as UMAP) have various limitations for interpretability. 
The coordinates of UMAP output for any given cell can change dramatically depending on parameters, and even run to run with the same parameters. 
This probably means that you shouldn't rely too heavily on the exact values of UMAP's output. 

    - One particular limitation of UMAP and t-SNE is that while observed clusters have some meaning, the distance *between* clusters usually does not (nor does cluster density). 
    The fact that two clusters are near each other should NOT be interpreted to mean that they are more related to each other than to more distant clusters. 
    (There is some disagreement about whether UMAP distances have more meaning, but it is probably safer to assume they don't.)
    
2. Different analyses also have their strengths. 
Using cell-type labeling, our experiment illustrated that UMAP does appear to give some biologically relevant output information for this dataset. 

3. Playing with parameters so you can fine-tune them is a good way to give you more information about a particular analysis as well as the data itself. 
   
In summary, if the results of an analysis can be completely changed by changing its parameters, you should be more cautious when it comes to the conclusions you draw from it as well as having good rationale for the parameters you choose. 

### t-SNE comparison

In the block below is a similar analysis and plot with t-SNE (t-distributed Stochastic Neighbor Embedding).
Note that this analysis also does PCA before moving on to the fancy machine learning.

```{r tsne}
# Run TSNE
filtered_sce <- runTSNE(filtered_sce)

# plot with builtin function
plotReducedDim(filtered_sce, "TSNE", colour_by = "label") + theme_bw()
```


```{r plot_tsne}

# Plot this with ggplot2 
ggplot(hodgkins_tsne, 
       aes(x = TSNE1, y = TSNE2, color = mito_percent)) + 
  geom_point(alpha = 0.1) +
  scale_color_viridis_c() + 
  theme_bw()
```

Different! (Slower!) Is it better or worse? Hard to say!
Different people like different things, and one plot might illustrate a particular point better than another. 

### Some further reading on dimension reduction:  

- This website explains [PCA visually](http://setosa.io/ev/principal-component-analysis/).  
- [Becht *et al.* (2018)](https://www.nature.com/articles/nbt.4314) discusses using [UMAP](https://github.com/lmcinnes/umap) for single-cell data.  
- [Wattenberg *et al.* (2016)](https://distill.pub/2016/misread-tsne/) discuss how to use t-SNE properly with great visuals. 
(The lessons apply to UMAP as well, with a broad substitution of the `n_neighbors` parameter for `perplexity`.)
- [Nguyen & Holmes (2019)](https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1006907&type=printable) lay out guidelines on choosing dimensions reduction methods.  
- [Freitag (2019)](https://rpubs.com/Saskia/520216) is a nice explanation and comparison of many different dimensionality reduction techniques that you may encounter.


## Session Info:

```{r}
sessionInfo()
```
